{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "edeebe15-81dc-405e-93fc-d529bed75f13",
      "metadata": {
        "id": "edeebe15-81dc-405e-93fc-d529bed75f13"
      },
      "source": [
        "## Instruction Tuning\n",
        "\n",
        "Supervised fine tuning (SFT) is fine-tuning all of a model’s parameters on supervised data of inputs and outputs. It teaches the model how to follow user specified instructions. It is typically done after model pre-training. **Source**: http://tinyurl.com/2v884put\n",
        "\n",
        "![instruction tuning](assets/instruction-tuning.jpg)\n",
        "\n",
        "Image Source: https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3\n",
        "\n",
        "Requirement.\n",
        "1. Pre-trained model & tokenizer -> We will get it from huggingface.\n",
        "2. Instruction-Response pair data -> eg: Alpaca, Dolly, Oasst1, LIMA, etc. We will get the dataset from huggingface.\n",
        "\n",
        "Steps.\n",
        "1. Load pre-trained model and tokenizer.\n",
        "2. Format the instructions response pair.\n",
        "3. Preprocess the dataset.\n",
        "4. Train the pre-trained model in supervised setting with response as labels and instruction as input.\n",
        "5. Evaluation:\n",
        "   i. Automatic Evaluation: Eg: MMLU, BBH, AGIEval, domain-specific evaluation such as maths, reasoning, code.\n",
        "   ii. Human Evaluation: Give model prompts to generate a response and ask humans.\n",
        "   iii. LLM as Evaluator: Ask powerful models such as GPT4 to rate the response generated by the your finetuned model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e36884d-9716-4f46-8259-74571b90def1",
      "metadata": {
        "id": "4e36884d-9716-4f46-8259-74571b90def1"
      },
      "source": [
        "#### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install --quiet datasets accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isqPaGANPygQ",
        "outputId": "04e64238-5838-4e04-9b86-6a7851aacb78"
      },
      "id": "isqPaGANPygQ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.35.2\n",
            "Uninstalling transformers-4.35.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.35.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled transformers-4.35.2\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-pnccyhqi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-pnccyhqi\n",
            "  Resolved https://github.com/huggingface/transformers to commit ebc8f47bd922734ce811b8b23c495e653c60afc9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2023.11.17)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8407964 sha256=4440108e6c178a2b596fe1a478fccd3c7d9100c8d2c50f85ff941427d9b8d780\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-idzzwgka/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.38.0.dev0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b",
      "metadata": {
        "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import os\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "from datasets import load_dataset\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "hXzhWVhQQdz3"
      },
      "id": "hXzhWVhQQdz3",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "Ug8sZg4PQpXX"
      },
      "id": "Ug8sZg4PQpXX",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")"
      ],
      "metadata": {
        "id": "DDj14nsdQrtF"
      },
      "id": "DDj14nsdQrtF",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f",
      "metadata": {
        "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f"
      },
      "source": [
        "##### 1. Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d5dbf5-b851-4307-85f4-eea519349249",
      "metadata": {
        "id": "13d5dbf5-b851-4307-85f4-eea519349249"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3",
      "metadata": {
        "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"gpt2\" #\"microsoft/phi-1_5\" # huggingface model name\n",
        "dataset_name_or_path = \"xzuyn/lima-alpaca\" # LIMA Data in Vicuna Format. https://arxiv.org/abs/2305.11206\n",
        "cache_dir=\"cache_dir\"\n",
        "split_name=\"train\"\n",
        "inst_col_name=\"instruction\"\n",
        "input_col_name=\"input\"\n",
        "output_col_name=\"output\"\n",
        "model_max_length=512 # how long sequence model can process\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a",
      "metadata": {
        "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a"
      },
      "source": [
        "##### 2. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "19cc1420-c916-44f2-9262-a34e3ca982f6",
      "metadata": {
        "id": "19cc1420-c916-44f2-9262-a34e3ca982f6"
      },
      "outputs": [],
      "source": [
        "# dataset = load_dataset(dataset_name_or_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is two prompt template / or wrapper we are going to use.\n",
        "- Some instruction contains\n"
      ],
      "metadata": {
        "id": "oX-KNklNzxrr"
      },
      "id": "oX-KNklNzxrr"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306",
      "metadata": {
        "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306"
      },
      "outputs": [],
      "source": [
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"prompt_no_input\":(\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe",
      "metadata": {
        "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1",
      "metadata": {
        "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1"
      },
      "source": [
        "If we are adding any new tokens to then we need to extend the embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0",
      "metadata": {
        "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0"
      },
      "outputs": [],
      "source": [
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c",
      "metadata": {
        "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c"
      },
      "outputs": [],
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8",
      "metadata": {
        "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8"
      },
      "outputs": [],
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bb0f0280-68e5-44f5-8494-71d793a66c1c",
      "metadata": {
        "id": "bb0f0280-68e5-44f5-8494-71d793a66c1c"
      },
      "outputs": [],
      "source": [
        "# tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "# tokenizer.pad_token=tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c062b8e5-631c-4a06-9859-8f9b985127af",
      "metadata": {
        "id": "c062b8e5-631c-4a06-9859-8f9b985127af"
      },
      "outputs": [],
      "source": [
        "# source = [dataset[split_name][inst_col_name][0]]\n",
        "# target = [dataset[split_name][output_col_name][0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "76742280-f2cc-41bc-a8b1-c6e7be1fd8af",
      "metadata": {
        "id": "76742280-f2cc-41bc-a8b1-c6e7be1fd8af"
      },
      "outputs": [],
      "source": [
        "# outputs = preprocess(sources=source, targets=target, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "807457e4-77ec-43b8-b1d4-15777089fc88",
      "metadata": {
        "id": "807457e4-77ec-43b8-b1d4-15777089fc88"
      },
      "outputs": [],
      "source": [
        "# len(outputs['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f3a2ccca-33c8-430b-a4c6-fedd3f01dc09",
      "metadata": {
        "id": "f3a2ccca-33c8-430b-a4c6-fedd3f01dc09"
      },
      "outputs": [],
      "source": [
        "# len(outputs['labels'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2666a9bf-8fc4-4532-8293-bc581e34cdf3",
      "metadata": {
        "id": "2666a9bf-8fc4-4532-8293-bc581e34cdf3"
      },
      "outputs": [],
      "source": [
        "# outputs['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50",
      "metadata": {
        "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50"
      },
      "outputs": [],
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name_or_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "\n",
        "        # Load the dataset\n",
        "        logging.warning(\"Loading data...\")\n",
        "        dataset = datasets.load_dataset(dataset_name_or_path, split=split_name)\n",
        "\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        # if there is no input for prompt the use prompt_no_input template else use prompt_input template\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(input_col_name, \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in tqdm(dataset)\n",
        "        ]\n",
        "        targets = [f\"{example[output_col_name]}{tokenizer.eos_token}\" for example in dataset]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "164bd107-ad15-4951-beb0-47442a072be7",
      "metadata": {
        "id": "164bd107-ad15-4951-beb0-47442a072be7"
      },
      "outputs": [],
      "source": [
        "# for example in dataset['train']:\n",
        "#     print(example.get(input_col_name, \"input\"))\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f7e0e317-c18e-450e-bb54-666150c17c5e",
      "metadata": {
        "id": "f7e0e317-c18e-450e-bb54-666150c17c5e"
      },
      "outputs": [],
      "source": [
        "# dataset = SupervisedDataset(dataset_name_or_path=dataset_name_or_path, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "L-lgnXWf0lRD"
      },
      "id": "L-lgnXWf0lRD",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLTcAZUU0ueb"
      },
      "id": "dLTcAZUU0ueb",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e",
      "metadata": {
        "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e"
      },
      "outputs": [],
      "source": [
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, dataset_name_or_path=dataset_name_or_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
      ],
      "metadata": {
        "id": "hvi5uWBa1emE"
      },
      "id": "hvi5uWBa1emE",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c",
      "metadata": {
        "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c"
      },
      "outputs": [],
      "source": [
        "def train(training_args):\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        cache_dir=cache_dir,\n",
        "    )\n",
        "    model.config.use_cache=False\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        cache_dir=cache_dir,\n",
        "        model_max_length=model_max_length,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "    if \"llama\" in model_name_or_path:\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer)\n",
        "\n",
        "    # update training args to make output dir\n",
        "    output_dir = os.path.join(training_args.output_dir, model_name_or_path.split(\"/\")[-1])\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    training_args.output_dir = output_dir\n",
        "\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "\n",
        "    # resume from last checkpoint if it exists\n",
        "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "\n",
        "    if checkpoint:\n",
        "        print(f\"Checkpoint found! Training from {checkpoint} checkpoint!\")\n",
        "        trainer.train(resume_from_checkpoint=checkpoint)\n",
        "    else:\n",
        "        print(f\"No checkpoint found! Training from scratch!\")\n",
        "        trainer.train()\n",
        "\n",
        "    # trainer.train()\n",
        "    # save states\n",
        "    trainer.save_state()\n",
        "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
        "    print(f\"Training finished! Saved model to {training_args.output_dir}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28933e42-bd69-4939-84e3-8a40ee361b89",
      "metadata": {
        "id": "28933e42-bd69-4939-84e3-8a40ee361b89"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"output\""
      ],
      "metadata": {
        "id": "aBHgmBOO1VWD"
      },
      "id": "aBHgmBOO1VWD",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = transformers.TrainingArguments(output_dir=output_dir, per_device_train_batch_size=2)"
      ],
      "metadata": {
        "id": "PuMQtT6U1Qt1"
      },
      "id": "PuMQtT6U1Qt1",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
        "outputId": "69ba995a-1cfd-453d-9fcd-8817f683ee5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 26628.64it/s]\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found! Training from scratch!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 08:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.061400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.748600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.599100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished! Saved model to output/gpt2.\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "train(training_args=training_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DePLcLnOWiGQ"
      },
      "id": "DePLcLnOWiGQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48577776-fdcb-4bed-ad61-4c7e79d774a7",
      "metadata": {
        "id": "48577776-fdcb-4bed-ad61-4c7e79d774a7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db9e526-ce4f-45c5-9fa8-70563a953a0f",
      "metadata": {
        "id": "2db9e526-ce4f-45c5-9fa8-70563a953a0f"
      },
      "outputs": [],
      "source": [
        "# training_args.num_train_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6",
      "metadata": {
        "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b",
      "metadata": {
        "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c3182e47-0897-41ae-821e-1b3b74c512a5",
      "metadata": {
        "id": "c3182e47-0897-41ae-821e-1b3b74c512a5"
      },
      "outputs": [],
      "source": [
        "# clone llm-evaluation-harness\n",
        "# !git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "# !cd lm-evaluation-harness\n",
        "# !pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XggnQZFSWpjl"
      },
      "id": "XggnQZFSWpjl"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
        "outputId": "6e653038-6d82-4f5d-9f2f-09fcc0214232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-23:17:27:18,744 INFO     [utils.py:160] NumExpr defaulting to 2 threads.\n",
            "2024-01-23:17:27:19,146 INFO     [config.py:58] PyTorch version 2.1.0+cu121 available.\n",
            "2024-01-23:17:27:19,147 INFO     [config.py:95] TensorFlow version 2.15.0 available.\n",
            "2024-01-23:17:27:19,148 INFO     [config.py:108] JAX version 0.4.23 available.\n",
            "2024-01-23 17:27:19.862024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 17:27:19.862115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 17:27:19.863642: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 17:27:21.074092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 13.7MB/s]\n",
            "2024-01-23:17:27:25,293 INFO     [__main__.py:156] Verbosity set to INFO\n",
            "2024-01-23:17:27:30,923 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
            "2024-01-23:17:27:35,102 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
            "2024-01-23:17:27:35,103 INFO     [__main__.py:229] Selected Tasks: ['hellaswag']\n",
            "2024-01-23:17:27:35,112 INFO     [huggingface.py:148] Using device 'cuda:0'\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Downloading builder script: 100% 4.36k/4.36k [00:00<00:00, 12.7MB/s]\n",
            "Downloading metadata: 100% 2.53k/2.53k [00:00<00:00, 13.5MB/s]\n",
            "Downloading readme: 100% 6.84k/6.84k [00:00<00:00, 22.4MB/s]\n",
            "Downloading data: 47.5MB [00:00, 53.2MB/s]\n",
            "Downloading data: 11.8MB [00:00, 46.9MB/s]\n",
            "Downloading data: 12.2MB [00:00, 50.0MB/s]\n",
            "Generating train split: 100% 39905/39905 [00:04<00:00, 8643.51 examples/s]\n",
            "Generating test split: 100% 10003/10003 [00:01<00:00, 8471.27 examples/s]\n",
            "Generating validation split: 100% 10042/10042 [00:01<00:00, 8885.95 examples/s]\n",
            "Map: 100% 39905/39905 [00:07<00:00, 5610.09 examples/s]\n",
            "Map: 100% 10042/10042 [00:01<00:00, 6953.01 examples/s]\n",
            "2024-01-23:17:28:01,675 INFO     [task.py:340] Building contexts for task on rank 0...\n",
            "2024-01-23:17:28:09,145 INFO     [evaluator.py:319] Running loglikelihood requests\n",
            "  0% 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            " 25% 9921/40168 [01:38<04:44, 106.15it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "100% 40168/40168 [04:45<00:00, 140.77it/s]\n",
            "hf (pretrained=/content/output/gpt2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (64,64,64,64)\n",
            "|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|\n",
            "|---------|------:|------|-----:|--------|-----:|---|-----:|\n",
            "|hellaswag|      1|none  |     0|acc     |0.2928|±  |0.0045|\n",
            "|         |       |none  |     0|acc_norm|0.3116|±  |0.0046|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=/content/output/gpt2 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05eadf1f-7b96-4799-a550-d166ff487d76",
      "metadata": {
        "id": "05eadf1f-7b96-4799-a550-d166ff487d76"
      },
      "source": [
        "### Prompting\n",
        "- Once we have trained the model to follow instructions, we can prompt that model to generate a response.\n",
        "- We will be using HF's generation pipeline to prompt our trained model.\n",
        "- After training the model with a particular prompt wrapper it is advised to use the same prompt format during inference.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1",
      "metadata": {
        "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f",
      "metadata": {
        "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and trained model and then create chatbot pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output/gpt2\", device_map=\"cuda:0\")\n",
        "\n",
        "chatbot = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format the prompt\n",
        "text = \"What is Machine Learning?\"\n",
        "\n",
        "prompt = PROMPT_DICT['prompt_no_input'].format(instruction=text)"
      ],
      "metadata": {
        "id": "EhL-B70DXjS1"
      },
      "id": "EhL-B70DXjS1",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3Ma2Y_-YR5A"
      },
      "id": "t3Ma2Y_-YR5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
        "outputId": "3800137f-215a-4026-d725-f2880e7aa316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "sequences = chatbot(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_p=0.4,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=64,\n",
        "    return_full_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ps ux"
      ],
      "metadata": {
        "id": "Qg79HroWZe7Y"
      },
      "id": "Qg79HroWZe7Y",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
        "outputId": "48d8c13d-e5b8-49a7-8dab-90741d633802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is a new field that has been around for a while. It is a field that has been around for a long time, and has been around for a long time. Machine learning is a new field that has been around for a long time, and has been around for a long time.\n",
            "\n",
            "## Machine\n"
          ]
        }
      ],
      "source": [
        "print(sequences[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26",
      "metadata": {
        "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "53f136b1-574a-46ff-8ade-5565db02229e",
      "metadata": {
        "id": "53f136b1-574a-46ff-8ade-5565db02229e"
      },
      "source": [
        "#### Next Tutorial - Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e0a0fe-4416-481b-8d03-b9cb89889204",
      "metadata": {
        "id": "01e0a0fe-4416-481b-8d03-b9cb89889204"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}